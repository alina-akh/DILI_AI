{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nest-asyncio aiohttp\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_chembl_data(df) -> List[Dict[str, Any]]:\n",
    "    processed_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        compound_data = {\"name\": \"\", \"smiles\": \"\", \"text\": \"\"}\n",
    "        if pd.notna(row.get('Molecule Name')):\n",
    "            compound_data[\"name\"] = str(row['Molecule Name'])\n",
    "        elif pd.notna(row.get('Molecule ChEMBL ID')):\n",
    "            compound_data[\"name\"] = f\"Compound_{row['Molecule ChEMBL ID']}\"\n",
    "        if pd.notna(row.get('Smiles')):\n",
    "            compound_data[\"smiles\"] = str(row['Smiles'])\n",
    "        text_parts = []\n",
    "        mol_info = []\n",
    "        if pd.notna(row.get('Molecule ChEMBL ID')):\n",
    "            mol_info.append(f\"ChEMBL ID: {row['Molecule ChEMBL ID']}\")\n",
    "        if pd.notna(row.get('Molecular Weight')):\n",
    "            mol_info.append(f\"Molecular Weight: {row['Molecular Weight']}\")\n",
    "        if pd.notna(row.get('#RO5 Violations')):\n",
    "            mol_info.append(f\"RO5 Violations: {row['#RO5 Violations']}\")\n",
    "        if pd.notna(row.get('AlogP')):\n",
    "            mol_info.append(f\"AlogP: {row['AlogP']}\")\n",
    "        if mol_info:\n",
    "            text_parts.append(\"Molecular properties: \" + \"; \".join(mol_info))\n",
    "        assay_info = []\n",
    "        if pd.notna(row.get('Assay Description')):\n",
    "            assay_info.append(f\"Assay: {row['Assay Description']}\")\n",
    "        if pd.notna(row.get('Assay Type')):\n",
    "            assay_info.append(f\"Type: {row['Assay Type']}\")\n",
    "        if pd.notna(row.get('Assay Organism')):\n",
    "            assay_info.append(f\"Organism: {row['Assay Organism']}\")\n",
    "        if pd.notna(row.get('Assay Tissue Name')):\n",
    "            assay_info.append(f\"Tissue: {row['Assay Tissue Name']}\")\n",
    "        if assay_info:\n",
    "            text_parts.append(\"Experimental details: \" + \"; \".join(assay_info))\n",
    "        activity_info = []\n",
    "        if pd.notna(row.get('Standard Type')):\n",
    "            activity_info.append(f\"Measurement type: {row['Standard Type']}\")\n",
    "        if pd.notna(row.get('Standard Relation')):\n",
    "            activity_info.append(f\"Relation: {row['Standard Relation']}\")\n",
    "        if pd.notna(row.get('Standard Value')):\n",
    "            activity_info.append(f\"Value: {row['Standard Value']}\")\n",
    "        if pd.notna(row.get('Standard Units')):\n",
    "            activity_info.append(f\"Units: {row['Standard Units']}\")\n",
    "        if pd.notna(row.get('pChEMBL Value')):\n",
    "            activity_info.append(f\"pChEMBL: {row['pChEMBL Value']}\")\n",
    "        if pd.notna(row.get('Standard Text Value')):\n",
    "            activity_info.append(f\"Text value: {row['Standard Text Value']}\")\n",
    "        if pd.notna(row.get('Value')):\n",
    "            activity_info.append(f\"Additional value: {row['Value']}\")\n",
    "        toxicity_info = []\n",
    "        if pd.notna(row.get('Data Validity Comment')):\n",
    "            toxicity_info.append(f\"Validity: {row['Data Validity Comment']}\")\n",
    "        if pd.notna(row.get('Comment')):\n",
    "            toxicity_info.append(f\"Comment: {row['Comment']}\")\n",
    "        if pd.notna(row.get('Action Type')):\n",
    "            toxicity_info.append(f\"Action: {row['Action Type']}\")\n",
    "        if toxicity_info:\n",
    "            activity_info.extend(toxicity_info)\n",
    "        if activity_info:\n",
    "            text_parts.append(\"Activity/toxicity data: \" + \"; \".join(activity_info))\n",
    "        target_info = []\n",
    "        if pd.notna(row.get('Target Name')):\n",
    "            target_info.append(f\"Target: {row['Target Name']}\")\n",
    "        if pd.notna(row.get('Target Organism')):\n",
    "            target_info.append(f\"Target organism: {row['Target Organism']}\")\n",
    "        if pd.notna(row.get('Target Type')):\n",
    "            target_info.append(f\"Target type: {row['Target Type']}\")\n",
    "        if target_info:\n",
    "            text_parts.append(\"Target information: \" + \"; \".join(target_info))\n",
    "        ligand_efficiency = []\n",
    "        if pd.notna(row.get('Ligand Efficiency BEI')):\n",
    "            ligand_efficiency.append(f\"BEI: {row['Ligand Efficiency BEI']}\")\n",
    "        if pd.notna(row.get('Ligand Efficiency LE')):\n",
    "            ligand_efficiency.append(f\"LE: {row['Ligand Efficiency LE']}\")\n",
    "        if pd.notna(row.get('Ligand Efficiency LLE')):\n",
    "            ligand_efficiency.append(f\"LLE: {row['Ligand Efficiency LLE']}\")\n",
    "        if pd.notna(row.get('Ligand Efficiency SEI')):\n",
    "            ligand_efficiency.append(f\"SEI: {row['Ligand Efficiency SEI']}\")\n",
    "        if ligand_efficiency:\n",
    "            text_parts.append(\"Ligand efficiency: \" + \"; \".join(ligand_efficiency))\n",
    "        compound_data[\"text\"] = \". \".join(text_parts)\n",
    "        if compound_data[\"text\"].strip():\n",
    "            processed_data.append(compound_data)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "API_KEYS = [\n",
    "    \"PXE3aM9RwSWVdyTQk0kgMk8fq0NotEca\",\n",
    "    \"v3efxoWTMOsWUvC9Etz7GVQjgsVwrq5P\",\n",
    "    \"E9v3G6uEBzZX5ZZYdBf0fr0ghMvjXIbG\",\n",
    "    \"eAcAMU0tziYMEhs31LGsU9VQKE99h9dM\",\n",
    "    \"K5OfEybaZkmukyJIxUfUnjuXA2Huz6Gy\",\n",
    "    \"qvmvzUl4jQqL5T6tUVUZNVTnvPXtG9yz\",\n",
    "    \"J1U0dl5cHzsCpzI10WGdTmpeMUKSjdm9\",\n",
    "    \"PDBy1PZniHZJXk4PEFsMQdZ7zCQT8Vw6\",\n",
    "    \"PGU9U73JlwXkoQWqdbsJUBK4ciQzp1Yq\",\n",
    "] \n",
    "\n",
    "MODEL_NAME = \"mistral-small-latest\"  \n",
    "MAX_RETRIES = 3\n",
    "BASE_DELAY = 5.0  \n",
    "BATCH_PAUSE = 1.0  # pause between questions\n",
    "\n",
    "# --- format prompt ---\n",
    "def build_prompt(compound_name: str, smiles: str, text: str) -> str:\n",
    "    system_prompt = \"\"\"You are an expert toxicologist. Extract ALL toxicity-related measurements from the text.\n",
    "CRITICAL: For each metric, SEPARATE the numerical value from the units.\n",
    "- Extract numerical values as floats when possible\n",
    "- Put units in separate field\n",
    "- If no clear numerical value, use descriptive text\n",
    "Examples:\n",
    "- \"LD50 of 500 mg/kg\" → value: 500, units: \"mg/kg\"\n",
    "- \"IC50 = 250 μM\" → value: 250, units: \"μM\" \n",
    "- \"albumin level 1.45 ± 0.06 g per 100 mL\" → value: 1.45, units: \"g/100mL\"\n",
    "- \"mild hepatotoxicity\" → value: \"mild\", units: null\n",
    "Toxicity levels (example for LD50):\n",
    "- \"none\": No toxicity data\n",
    "- \"low\": High LD50 (>1000 mg/kg), minimal effects\n",
    "- \"moderate\": LD50 50-1000 mg/kg, dose-dependent  \n",
    "- \"high\": LD50 10-50 mg/kg, significant toxicity\n",
    "- \"severe\": LD50 <10 mg/kg, lethal damage\n",
    "Respond ONLY in valid JSON matching this schema:\n",
    "{\n",
    "  \"compound_name\": \"...\",\n",
    "  \"smiles\": \"...\",\n",
    "  \"toxicity_level\": \"...\",\n",
    "  \"evidence\": \"...\",\n",
    "  \"toxicity_metrics\": { \"...\": { \"value\": ..., \"units\": \"...\", \"description\": \"...\" } },\n",
    "  \"confidence\": \"...\"\n",
    "}\"\"\"\n",
    "    user_prompt = f\"\"\"Compound: {compound_name}\n",
    "SMILES: {smiles}\n",
    "Text:\n",
    "{text}\n",
    "Extract and structure all toxicity metrics:\"\"\"\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "# --- requests to Mistral API ---\n",
    "async def call_mistral(\n",
    "    session: aiohttp.ClientSession,\n",
    "    api_key: str,\n",
    "    compound_name: str,\n",
    "    smiles: str,\n",
    "    text: str,\n",
    "    retry_count: int = 0\n",
    ") -> Dict[str, Any]:\n",
    "    system, user = build_prompt(compound_name, smiles, text)\n",
    "    url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 500,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async with session.post(url, headers=headers, json=payload) as resp:\n",
    "            if resp.status == 200:\n",
    "                data = await resp.json()\n",
    "                content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                parsed = json.loads(content)\n",
    "                return parsed\n",
    "            elif resp.status == 429:\n",
    "                if retry_count < MAX_RETRIES:\n",
    "                    wait = BASE_DELAY * (2 ** retry_count) + random.uniform(0, 1)\n",
    "                    print(f\"429 error. Retrying in {wait:.2f}s (attempt {retry_count + 1})\")\n",
    "                    await asyncio.sleep(wait)\n",
    "                    return await call_mistral(session, api_key, compound_name, smiles, text, retry_count + 1)\n",
    "                else:\n",
    "                    print(\"Max retries exceeded for 429 error.\")\n",
    "                    return {\"error\": \"429 after max retries\"}\n",
    "            else:\n",
    "                text_resp = await resp.text()\n",
    "                print(f\"HTTP {resp.status}: {text_resp}\")\n",
    "                return {\"error\": f\"HTTP {resp.status}\", \"response\": text_resp}\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during API call: {e}\")\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            await asyncio.sleep(BASE_DELAY * (2 ** retry_count))\n",
    "            return await call_mistral(session, api_key, compound_name, smiles, text, retry_count + 1)\n",
    "        else:\n",
    "            return {\"error\": \"Exception after max retries\", \"exception\": str(e)}\n",
    "\n",
    "\n",
    "async def process_batch(test_cases: List[Dict[str, str]], output_file: str):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i in range(0, len(test_cases), 9):\n",
    "            batch = test_cases[i:i+9]\n",
    "            tasks = []\n",
    "            for j, case in enumerate(batch):\n",
    "                key = API_KEYS[j % len(API_KEYS)]\n",
    "                task = call_mistral(\n",
    "                    session,\n",
    "                    key,\n",
    "                    case[\"name\"],\n",
    "                    case[\"smiles\"],\n",
    "                    case[\"text\"]\n",
    "                )\n",
    "                tasks.append(task)\n",
    "\n",
    "            print(f\"Sending batch {i//9 + 1} with {len(tasks)} requests...\")\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            # Запись результатов в файл\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                for res in results:\n",
    "                    f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            print(f\"Batch {i//9 + 1} completed. Pausing for {BATCH_PAUSE}s...\")\n",
    "            await asyncio.sleep(BATCH_PAUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"input/toxicity/chembl.csv\", sep=\";\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "test_cases = preprocess_chembl_data(df.iloc[40_000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(process_batch(test_cases, \"working/chembl_40000-48375.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8500305,
     "sourceId": 13395281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8500347,
     "sourceId": 13395339,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
